{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "\n",
    "import collections\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import random\n",
    "from tempfile import gettempdir\n",
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "from six.moves import urllib\n",
    "from six.moves import xrange\n",
    "import tensorflow as tf\n",
    "from utilities import percentage\n",
    "\n",
    "from tensorflow.contrib.tensorboard.plugins import projector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "current_path=os.path.dirname(os.path.realpath(sys.argv[0]))\n",
    "parser=argparse.ArgumentParser()\n",
    "parser.add_argument(\n",
    "    \"--log_dir\",\n",
    "    type=str,\n",
    "    default=os.path.join(current_path,\"log\"),\n",
    "    help=\"log directory for tensorboard\"\n",
    ")\n",
    "FLAGS,unparsed=parser.parse_known_args()\n",
    "\n",
    "if not os.path.exists(FLAGS.log_dir):\n",
    "    os.path.mkdirs(FLAGS.log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "# step-1 download the data\n",
    "url=\"http://mattmahoney.net/dc/\"\n",
    "\n",
    "def maybe_download(filename,expected_bytes):\n",
    "    \"\"\"download a file and confirm the size\"\"\"\n",
    "    local_filename=os.path.join(gettempdir(),filename)\n",
    "    if not os.path.exists(local_filename):\n",
    "        local_filename,_=urllib.request.urlretrieve(url+filename,local_filename)\n",
    "        \n",
    "    statinfo=os.stat(local_filename)\n",
    "    if statinfo.st_size==expected_bytes:\n",
    "        print(\"Found and verified\",filename)\n",
    "    else:\n",
    "        print(statinfo.st_size)\n",
    "        raise Exception(\"Failed to verify \"+local_filename+\". can you get it with a browser\")\n",
    "    return local_filename\n",
    "\n",
    "\n",
    "filename=maybe_download('text8.zip',31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read the data into a list of strings\n",
    "def read_data(filename):\n",
    "    \"\"\"etract the first file enclosed in a zip file as a list of words\"\"\"\n",
    "    with zipfile.ZipFile(filename) as f:\n",
    "        return tf.compat.as_str(f.read(f.namelist()[0])).split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# step 2:build the dictionary and replace rare words with UNK token\n",
    "vocabulary_size=50000\n",
    "vocabulary=read_data(filename)\n",
    "\n",
    "def build_dataset(words,n_words):\n",
    "    \"process raw inputs  into a dataset\"\n",
    "    unk_count=0\n",
    "    count=[('UNK',-1)]\n",
    "    count.extend(collections.Counter(words).most_common(n_words-1))\n",
    "    dictionary=dict()\n",
    "    for word,_  in count:\n",
    "        dictionary[word]=len(dictionary)\n",
    "    data=list()\n",
    "    \n",
    "    for word in words:\n",
    "        index=dictionary.get(word,0)\n",
    "        if index==0:\n",
    "            unk_count+=1\n",
    "        data.append(index)\n",
    "    count[0]=(\"UNK\",unk_count)\n",
    "    reversed_dictionary=dict(zip(dictionary.values(),dictionary.keys()))\n",
    "    return data,count,dictionary,reversed_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "most common words (+unk) [('in', 372201), ('a', 325873), ('to', 316376), ('zero', 264975), ('nine', 250430)]\n",
      "Sample data [5234, 3081, 12, 6, 195, 2, 3134, 46, 59, 156] ['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against']\n"
     ]
    }
   ],
   "source": [
    "data,count,dictionary,reverse_dictionary=build_dataset(vocabulary,vocabulary_size)\n",
    "del vocabulary\n",
    "print(\"most common words (+unk)\",count[5:10])\n",
    "print('Sample data', data[:10], [reverse_dictionary[i] for i in data[:10]])\n",
    "data_index=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3081 originated -> 5234 anarchism\n",
      "3081 originated -> 12 as\n",
      "12 as -> 3081 originated\n",
      "12 as -> 6 a\n",
      "6 a -> 195 term\n",
      "6 a -> 12 as\n",
      "195 term -> 2 of\n",
      "195 term -> 6 a\n"
     ]
    }
   ],
   "source": [
    "def generate_batch(batch_size,num_skips,skip_window):\n",
    "#     create global variables\n",
    "    global data_index\n",
    "    assert num_skips<=(skip_window*2)\n",
    "    assert batch_size%num_skips==0\n",
    "    \n",
    "# instantialize local variables\n",
    "    batch=np.ndarray(shape=(batch_size),dtype=np.int32)\n",
    "    labels=np.ndarray(shape=(batch_size,1),dtype=np.int32)\n",
    "    span=(2*skip_window)+1\n",
    "    buffer=collections.deque(maxlen=span)\n",
    "    \n",
    "    \n",
    "#     conditionals for data_index\n",
    "    if data_index+span>len(data):\n",
    "        data_index=0\n",
    "        \n",
    "#     initialize buffer\n",
    "    buffer.extend(data[data_index:data_index+span])\n",
    "    data_index+=span\n",
    "    \n",
    "#     major loop for batch generation\n",
    "    for i in range(batch_size//num_skips):\n",
    "        context_words=[word for word in range(span) if word!=skip_window]\n",
    "        words_to_use=random.sample(context_words,num_skips)\n",
    "        \n",
    "#         inner loop to put context words and labels in batch&labels\n",
    "        for j,word_to_use in enumerate(words_to_use):\n",
    "            batch[(i*num_skips)+j]=buffer[skip_window]\n",
    "            labels[(i*num_skips)+j,0]=buffer[word_to_use]\n",
    "        \n",
    "#         check the length of buffer array to make sure theres still space\n",
    "        if data_index==len(data):\n",
    "            buffer.extend(data[0:span])\n",
    "            data_index=span\n",
    "        else:\n",
    "            buffer.append(data[data_index])\n",
    "            data_index=data_index+1\n",
    "    \n",
    "#     backtrack data index a little to avoid missing words at the end of a batch\n",
    "    data_index=(data_index+len(data)-span)%len(data) \n",
    "    return batch,labels\n",
    "\n",
    "batch, labels = generate_batch(batch_size=8, num_skips=2, skip_window=1)\n",
    "for i in range(8):\n",
    "  print(batch[i], reverse_dictionary[batch[i]], '->', labels[i, 0],\n",
    "        reverse_dictionary[labels[i, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# defaination  of  variables\n",
    "\n",
    "batch_size=128\n",
    "embedding_size=128 #size of the dense embeddding vector\n",
    "skip_window=1 #number of times to reuse each word\n",
    "num_skips=2 #number of words to consider both sides\n",
    "num_sampled=64 #how many negative examples to sample\n",
    "\n",
    "valid_size=10\n",
    "valid_window=1000\n",
    "valid_examples=np.random.choice(valid_window,valid_size,replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph=tf.Graph()\n",
    "\n",
    "\n",
    "with graph.as_default():\n",
    "    \n",
    "#     input_data\n",
    "    with tf.name_scope('inputs'):\n",
    "        train_inputs=tf.placeholder(dtype=tf.int32,shape=[batch_size])\n",
    "        train_labels=tf.placeholder(dtype=tf.int32,shape=[batch_size,1])\n",
    "        valid_dataset=tf.constant(valid_examples,dtype=tf.int32)\n",
    "        \n",
    "    with tf.name_scope(\"embeddings\"):\n",
    "#         look up embeddings for inputs\n",
    "        embeddings=tf.Variable(\n",
    "            tf.random_uniform([vocabulary_size,embedding_size],-1.0,1)\n",
    "        )\n",
    "        embed=tf.nn.embedding_lookup(embeddings,train_inputs)\n",
    "        \n",
    "#         contruct the variables for the NCE loss\n",
    "    with tf.name_scope(\"weights\"):\n",
    "        nce_weights=tf.Variable(\n",
    "            tf.random_normal(\n",
    "                shape=[vocabulary_size,embedding_size],\n",
    "                stddev=1.0/math.sqrt(embedding_size)\n",
    "            ))\n",
    "    with tf.name_scope(\"biases\"):\n",
    "        nce_biases=tf.Variable(tf.zeros(shape=[vocabulary_size]))\n",
    "        \n",
    "    with tf.name_scope(\"loss\"):\n",
    "        loss=tf.reduce_mean(\n",
    "            tf.nn.nce_loss(\n",
    "                weights=nce_weights,\n",
    "                biases=nce_biases,\n",
    "                labels=train_labels,\n",
    "                inputs=embed,\n",
    "                num_sampled=num_sampled,\n",
    "                num_classes=vocabulary_size\n",
    "            )\n",
    "        )\n",
    "    tf.summary.scalar('loss',loss)\n",
    "    \n",
    "    with tf.name_scope('optimizer'):\n",
    "        \n",
    "        optimizer=tf.train.AdamOptimizer(0.0001).minimize(loss)\n",
    "#         creatingasaver\n",
    "    init=tf.global_variables_initializer()\n",
    "    saver=tf.train.Saver()\n",
    "    \n",
    "    norm=tf.sqrt(tf.reduce_sum(tf.square(embeddings),1,keepdims=True))\n",
    "    normalized_embeddings=embeddings/norm\n",
    "    \n",
    "    valid_embeddings=tf.nn.embedding_lookup(normalized_embeddings,valid_dataset)\n",
    "    \n",
    "    similarity=tf.matmul(\n",
    "        valid_embeddings,\n",
    "        normalized_embeddings,\n",
    "        transpose_b=True\n",
    "    )\n",
    "    \n",
    "    merged=tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /temp/tf_models/embedding.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /temp/tf_models/embedding.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialized\n",
      "epoch:0  loss:42.226627349853516\n",
      "top 5 closest to word to:set are ['country', 'beginning', 'settled', 'astronomy', 'theme']\n",
      "top 5 closest to word to:see are ['abu', 'articles', 'becomes', 'practice', 'era']\n",
      "top 5 closest to word to:movie are ['country', 'final', 'scholars', 'senses', 'duty']\n",
      "top 5 closest to word to:language are ['notably', 'whig', 'art', 'banquet', 'rand']\n",
      "top 5 closest to word to:al are ['torture', 'delays', 'discoveries', 'mortal', 'god']\n",
      "top 5 closest to word to:related are ['classical', 'win', 'secret', 'philosophy', 'discovered']\n",
      "top 5 closest to word to:described are ['powers', 'statistical', 'soon', 'result', 'inhabited']\n",
      "top 5 closest to word to:won are ['mid', 'help', 'almost', 'troy', 'promote']\n",
      "top 5 closest to word to:thomas are ['irritated', 'athenians', 'prime', 'works', 'defeat']\n",
      "top 5 closest to word to:close are ['help', 'studied', 'independence', 'behind', 'elected']\n",
      "93.50%\r"
     ]
    }
   ],
   "source": [
    "num_steps=100001\n",
    "percent=0\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    writer=tf.summary.FileWriter(FLAGS.log_dir,session.graph)\n",
    "    saver.restore(session,\"/temp/tf_models/embedding.ckpt\")\n",
    "#     init.run()\n",
    "    print(\"initialized\")\n",
    "    \n",
    "    average_loss=0\n",
    "    \n",
    "    for step in xrange(num_steps):\n",
    "        batch_inputs,batch_labels=generate_batch(batch_size,num_skips,skip_window)\n",
    "        feed_dict={train_inputs:batch_inputs,train_labels:batch_labels}\n",
    "        run_metadata=tf.RunMetadata()\n",
    "        _,summary,loss_val=session.run([optimizer,merged,loss],feed_dict=feed_dict,run_metadata=run_metadata)\n",
    "        saver.save(session,\"/temp/tf_models/embedding.ckpt\")\n",
    "        average_loss+=loss_val\n",
    "        percentage(percent,200)\n",
    "        percent=percent+1\n",
    "        writer.add_summary(summary,step);\n",
    "        \n",
    "        if step ==(num_steps-1):\n",
    "            writer.add_run_metadata(run_metadata,'step%d'%step)\n",
    "        \n",
    "        if not step%200:\n",
    "            percent=0\n",
    "            print(\"epoch:{}  loss:{}\".format(step,loss_val))\n",
    "            similarity_vector=session.run(similarity)\n",
    "            for i,word in enumerate(valid_examples):\n",
    "                print(\"top 5 closest to word to:{} are {}\".format(reverse_dictionary[word],[reverse_dictionary[word_index] for word_index in (-similarity_vector[i]).argsort()[1:5+1]]))\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_with_labels(low_dim_embs, labels, filename):\n",
    "  assert low_dim_embs.shape[0] >= len(labels), 'More labels than embeddings'\n",
    "  plt.figure(figsize=(18, 18))  # in inches\n",
    "  for i, label in enumerate(labels):\n",
    "    x, y = low_dim_embs[i, :]\n",
    "    plt.scatter(x, y)\n",
    "    plt.annotate(\n",
    "        label,\n",
    "        xy=(x, y),\n",
    "        xytext=(5, 2),\n",
    "        textcoords='offset points',\n",
    "        ha='right',\n",
    "        va='bottom')\n",
    "\n",
    "  plt.savefig(filename)\n",
    "\n",
    "\n",
    "try:\n",
    "  # pylint: disable=g-import-not-at-top\n",
    "  from sklearn.manifold import TSNE\n",
    "  import matplotlib.pyplot as plt\n",
    "\n",
    "  tsne = TSNE(\n",
    "      perplexity=30, n_components=2, init='pca', n_iter=5000, method='exact')\n",
    "  plot_only = 500\n",
    "  low_dim_embs = tsne.fit_transform(final_embeddings[:plot_only, :])\n",
    "  labels = [reverse_dictionary[i] for i in xrange(plot_only)]\n",
    "  plot_with_labels(low_dim_embs, labels, os.path.join(gettempdir(), 'tsne.png'))\n",
    "\n",
    "except ImportError as ex:\n",
    "  print('Please install sklearn, matplotlib, and scipy to show embeddings.')\n",
    "  print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([28756, 26935,  5963, ...,    71,    90,   464], dtype=int64)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity_vector[1].argsort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  464,    90,    71, ...,  5963, 26935, 28756], dtype=int64)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(-similarity_vector[1]).argsort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "bad operand type for unary -: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-3bf44a1cac8f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: bad operand type for unary -: 'list'"
     ]
    }
   ],
   "source": [
    "np.array([1,2,3,4]).argsort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
